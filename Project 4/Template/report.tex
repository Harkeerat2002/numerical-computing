\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}

\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}

\usepackage{amssymb} % for \mathbb
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}  % for align environment
\usepackage{graphicx}
\usepackage{float}
\usepackage{subfig}
\usepackage{dirtree}

\restylefloat{figure}

\setlength{\parindent}{0pt}


\lstdefinestyle{matlab}{
    language=Matlab,
    basicstyle=\ttfamily\small,
    commentstyle=\color[RGB]{34,139,34},
    keywordstyle=\color[RGB]{0,0,255},
    numberstyle=\tiny\color[RGB]{128,128,128},
    numbers=left,
    stepnumber=1,
    frame=single,
    breaklines=true,
    backgroundcolor=\color[RGB]{240,240,240},
    tabsize=2,
    columns=flexible,
    showstringspaces=false
}

\input{assignment.sty}
\begin{document}


\setassignment
\setduedate{Wednesday, 6 December 2023, 11:59 PM}

\serieheader{Numerical Computing}{2023}{\textbf{Student:} Harkeerat Singh Sawhney}{}
\newline

\assignmentpolicy


\newpage

\section{General Questions [10 points]}

\subsection{Size of Matrix A}
From the Background Information given for this Project we do know that $A \in \mathbb{R}^{n^2 \times n^2} $ indicates the transformation matrix coming from the repeated application of what is referred to as the "image kernel", which in our case tends to produce the blurring effect. In the other hand $B$ is the transformed blurred image and $X$ is the original square, grayscale image matrix, in which each matrix entry corresponds to one pixel value. Hence the blurring computation can be defined be following equation:

\begin{equation}
    Ax = b
\end{equation}

In the above equation $x$ and $b$ are the vectorized repersentation of $X$ and $B$ respectively.

\begin{lstlisting}[style=matlab, caption={Computing the size of A}, label={lst:computing_size_of_A}]
    %% Load Default Img Data
    load('blur_data/B.mat');
    B=double(B);
    n = size(B,1);
    sizeA = n * n;
    disp(['Size of A: ', num2str(sizeA)]);
\end{lstlisting}

From the Code Listing \ref{lst:computing_size_of_A} we can see that the size of $A$ is $62500 \times 62500$. This is computed through the size of $B$ which is $250 \times 250$ and then multiplying it by itself.

\subsection{How many diagonal bands does A have?}
It is understood that A is a $d^2$-banded symmetric matrix, where $d << n$. Since we know that the size of the kernel image matrix is $7 \times 7$ then we can also compute the amount of diagonal bands that A has. Hence the amount of diagonal bands that A has is $49$.

\subsection{What is the length of the vectorized blurred image b}
In order to compute the length of the vectorized blurred image $b$, we need to compute the size of $B$ and then multiply it by itself. We know that $B$ is a $250 \times 250$ matrix, hence the length of the vectorized blurred image $b$ is $62500$.

\section{Properties of A [10 points]}
\subsection{If A is not symmetric, how would this affect $\tilde{A}$?}
A is used to compute the Conjugate Gradient method, which is an iterative method to solve the linear system $Ax = b$. If A is symmetric of full rank but not positive-definite we can bypass this issue by solving the augmented System.

\begin{align}
    A^TAx      & = A^Tb      \\
    \tilde{A}x & = \tilde{b}
\end{align}

In the above equation the pre-multiplication with $A^T$ ensures that the resulting matrix $\tilde{A}$ is symmetric and positive-definite. Hence even if A is not symmetric, we can still compute $\tilde{A}$ because of the properties of $A^T$.

\subsection{Explain why solving $Ax = b$ for $x$ is equivalent to minimizing $\frac{1}{2} x^T Ax - b^Tx$ over $x$, assuming that $A$ is symmetric positive\textendash definite.}

We want to show that by minimizing $\frac{1}{2} x^T Ax - b^Tx$ over $x$ is equivalent to solving $Ax = b$. We can do this by taking the derivative of $\frac{1}{2} x^T Ax - b^Tx$ with respect to $x$ and setting it to zero. Hence we get the following equation:

\begin{align}
    \frac{d}{dx} \left( \frac{1}{2} x^T Ax - b^Tx \right) & = 0                           \\
    \frac{1}{2} \left( x^T A + x^T A^T \right) - b^T      & = 0                           \\
    x^T A - b^T                                           & = 0                           \\
    x^T A                                                 & = b^T                         \\
    x^T                                                   & = b^T A^{-1}                  \\
    x                                                     & = \left( b^T A^{-1} \right)^T \\
    x                                                     & = \left( A^{-1} \right)^T b   \\
    x                                                     & = A^{-1} b                    \\
    Ax                                                    & = b
\end{align}

Therefore can see at the end that we get the equation $Ax = b$ which is what we wanted to show.

\section{Conjugate Gradient [30 points]}


\subsection{Write a function for the conjugate gradient solver \texttt{[x, rvec] = myCG(A, b, x0, max\_itr, tol)}, where \texttt{x} and \texttt{rvec} are, respectively, the solution value and a vector containing the residual at every iteration.}

In this question we are asked to write a function for the conjugate gradient solver. The function is called \texttt{myCG} and it takes in the following parameters: \texttt{A}, \texttt{b}, \texttt{x0}, \texttt{max\_itr} and \texttt{tol}. The function returns the solution value \texttt{x} and a vector containing the residual at every iteration \texttt{rvec}. The function is implemented in the Code Listing \ref{lst:myCG}. The code is written from the provided Conjugate Gradient Algorithm in the project description.
\\
\\
The function first initializes the solution $x$ with the initial guess $x0$, and computes the initial residual $r$ as $b - A * x0$. the direction $d$ is also initialized as r. Then the function enters a for loop which iterates for the maximum number of iterations. In each iteration, it performs the steps which is provided in the algorithm.

\begin{lstlisting}[style=matlab, caption={Matlab function for Conjugate Gradient}, label={lst:myCG}]
    function [x, rvec] = myCG(A, b, x0, maxIter, tol)
        rvec = [];	
        x = x0;
        r = b - A * x0;
        d = r;
        pho_old = dot(r, r);
        
        
        for i = 1:maxIter
            s = A * d;
            alpha = pho_old / dot(d, s);
            x = x + alpha * d;
            r = r - alpha * s;
            pho_new = dot(r, r);

            beta = pho_new / pho_old;
            d = r + beta * d;
            pho_old = pho_new;
            
            rvec = [rvec, pho_new];

            if sqrt(pho_new) < tol
                disp('Converged');
                break;
            end
            
            
        end
    end
\end{lstlisting}

\subsection{In order to validate your implementation, solve the system defined by \texttt{A\_test.mat} and \texttt{b\_test.mat}. Plot the convergence (residual vs iteration)}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{graphs/residuals.png}
    \caption{Residuals vs Iterations for the Conjugate Gradient Algorithm}
    \label{fig:residuals}
\end{figure}

In this question we are asked to validate our implementation of the Conjugate Gradient Algorithm. We are given the matrix $A$ and the vector $b$. We are then asked to use the function which we implemented in the previous question to solve the system $Ax = b$. When we obtain the solution $x$ we are asked to plot the convergence of the residuals vs the iterations. The plot is shown in Figure \ref{fig:residuals}.
\\
\\
Our maximum number of iterations is set to 200 and the tolerance is set to $10^{-4}$. From the Figure \ref{fig:residuals} we can see that the residuals converge right before the 180th iteration. Also the from the plot we can see that the residual overall decreases, but does has multiple spikes in between.

\subsection{Plot the eigenvalues of A \texttt{test.mat} and comment on the condition number and convergence rate.}

In this question we are asked to plot the eigenvalues of $A$ and comment on the condition number and the convergence rate. Condition Number is defined as the ratio of the largest eigenvalue to the smallest eigenvalue. The condition number $\kappa(A)$ is the relation of sensitivy of the solution $x$ to the changes in the right hand side $b$. The condition number is computed as follows:

\begin{equation}
    \kappa(A) = \frac{\lambda_{max}}{\lambda_{min}}
\end{equation}

If small changes in $b$ cause large changes in $x$, then the system is called as ill-conditioned and the condition number of the system is large. However in the other hand, if small changes in $b$ cause small changes in $x$, then the system is called as well-conditioned and the condition number of the system is small. Therefore the condition number is a measure of the sensitivity of the solution $x$ to the changes in the right hand side $b$.
\\
\\
Hence in order to compute the condition number of $A$ we need to compute the eigenvalues of $A$. The eigenvalues of $A$ are computed using the \texttt{eig} function in Matlab. The eigenvalues are then sorted in ascending order and plotted. The plot is shown in Figure \ref{fig:eigenvalues}. As it can be seen in the plot the difference between the largest eigenvalue and the smallest eigenvalue is very large. Hence the condition number of $A$ is very large. This means that the system is ill-conditioned and small changes in $b$ will cause large changes in $x$. This can also be seen by calculating the condition number of $A$. We can do that by using the inbuilt \texttt{cond} function in Matlab. The condition number of $A$ is approximately $1.67 \times 10^{6}$ which is very large. Hence this is aligned with our previous observation that the system is ill-conditioned.



\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{graphs/eigenvalues.png}
    \caption{Eigenvalues of A}
    \label{fig:eigenvalues}
\end{figure}

\subsection{Does the residual decrease monotonically? Why or why not?}
As it can be seen from Figure \ref{fig:residuals} the residual does not decrease monotonically. The residual decreases overall but has multiple spikes in between. Therefore when we are computing the residuals, we are computing the difference between the actual solution $x$ and the computed solution $x$. Since the system is ill-conditioned, small changes in $b$ will cause large changes in $x$. Hence the residual will have multiple spikes in between, but will overall decrease.


\section{Deblurring problem [35 points]}
\subsection{Solve the deblurring problem for the blurred image matrix \texttt{B.mat} and transformation matrix \texttt{A.mat} using your routine \texttt{myCG} and Matlab’s preconditioned conjugate gradient \texttt{pcg}. As a preconditioner, use \texttt{ichol} to get the incomplete Cholesky factors and set routine type to \texttt{nofill} with $\alpha = 0.01$ for the diagonal shift (see Matlab documentation). Solve the system with both solvers using \texttt{max\_iter = 200} and \texttt{tol = 1e-6}. Plot the convergence (residual vs iteration) of each solver and display the original and final deblurred image. Comment on the results that you observe.}


In this question we have to solve the deblurring problem for the blurred image matrix \texttt{B.mat} and transformation matrix \texttt{A.mat} using our implemented \texttt{myCG} function and Matlab's \texttt{pcg} function. We are also asked to use \texttt{ichol} as a preconditioner for the \texttt{pcg} function
\\
\\
We first load the matrices $A$ and $B$. $A$ is the blurring matrix and $B$ is the blurred image. We then prepare for the Preconditioned Conjugate Gradient (PCG) method by creating the augmented system $augA$ and $augB$. We compute the preconditioner $M$ by using matlab function \texttt{ichol} with the \texttt{nofill} option (as asked). The PCG method is then applied to solved the system $augA \times x = augB$. We also make sure to have a diagonal shift of $\alpha = 0.01$ in the PCG method which is done by using the matlab function \texttt{speye} in which we create a sparse identity matrix and then multiply it by $\alpha$ and add that to our matrix $augA$. The solution $x$ is then reshaped to the original image size and then displayed.
\\
\\
As for the implementation with our \texttt{myCG} function, we call the function with the parameters $A$, $B$, $x0$, $maxIter$ and $tol$. With this we are able to solve the system $Ax = b$. For both the implementation the result is then reshaped to the original image size and then displayed. Also the parameters for both the implementations are set to $maxIter = 200$ and $tol = 1e-6$.

In the Code Listing \ref{lst:deblurring} we can see the implementation of the deblurring problem. The code follows the explanation given above.

\begin{lstlisting}[style=matlab, caption={Matlab code for solving the deblurring problem}, label={lst:deblurring}]
    %% Excercise 4.1
    % Load data
    loaded_A = load('blur_data/A.mat');
    loaded_B = load('blur_data/B.mat');

    A = loaded_A.A;
    B = loaded_B.B;


    img = B; % Blurred image
    n = size(img, 1);
    b = B(:); % Vectorized blurred image
    guess = ones(size(A, 1), 1);
    maxiter = 200; % Maximum number of iterations
    tol = 1e-6; % Tolerance

    % Display image
    imagesc(reshape(img, [n, n]));
    colormap('gray');
    axis off;
    saveas(gcf, '../Template/graphs/blurred.png');

    % Solve the system using 'pcg'
    augA = A' * A; % Augmented matrix of A
    augA_shifted = augA + 0.01 * speye(size(augA)); % Shifted matrix

    L = ichol(augA_shifted, struct('type', 'nofill')); % Incomplete Cholesky factorization
    M = L * L'; % Preconditioner
    M1 = L';
    M2 = L;
    augB = A' * b; % Augmented vector of b

    [x_pcg, flag, relres, iter, resvec_pcg] = pcg(augA, augB, tol, maxiter, M1, M2); % Solve the system


    % Draw deblurred image obtained with 'pcg'
    imagesc(reshape(x_pcg, [n, n]));
    colormap('gray');
    axis off;
    saveas(gcf, '../Template/graphs/deblurred_pcg.png');

    % Draw residuals vs iterations
    semilogy(resvec_pcg);
    xlabel('Iterations');
    ylabel('Residual value');
    legend('Residuals');
    title('Residuals vs Iterations for PCG');
    saveas(gcf, '../Template/graphs/residuals_pcg.png');

    disp(['Convergence flag: ', num2str(flag)]);

    % Solve the system using 'myCG'

    [x, residuals] = myCG(A, b, guess, maxiter, tol); % Solve the system

    % Draw deblurred image obtained with 'myCG'
    imagesc(reshape(x, [n, n]));
    colormap('gray');
    axis off;
    saveas(gcf, '../Template/graphs/deblurred_mycg.png');

    % Draw residuals vs iterations
    semilogy(residuals);
    xlabel('Iterations');
    ylabel('Residual value');
    legend('Residuals');
    title('Residuals vs Iterations for myCG');
    saveas(gcf, '../Template/graphs/residuals_mycg.png');
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{graphs/blurred.png}
    \caption{Blurred Image}
    \label{fig:blurred}
\end{figure}

\begin{figure}[H]
    \centering
    \subfloat[Deblurred Image using myCG]{\includegraphics[width=0.45\linewidth]{graphs/deblurred_mycg.png}\label{fig:deblurred_mycg}}
    \hfill
    \subfloat[Deblurred Image using PCG]{\includegraphics[width=0.45\linewidth]{graphs/deblurred_pcg.png}\label{fig:deblurred_pcg}}
    \caption{Comparison of Deblurred Images using myCG and PCG}
    \label{fig:deblurred_comparison}
\end{figure}


\begin{figure}[H]
    \centering
    \subfloat[Residuals vs Iterations for PCG]{\includegraphics[width=0.45\linewidth]{graphs/residuals_pcg.png}\label{fig:residuals_pcg}}
    \hfill
    \subfloat[Residuals vs Iterations for myCG]{\includegraphics[width=0.45\linewidth]{graphs/residuals_mycg.png}\label{fig:residuals_mycg}}
    \caption{Comparison of Residuals vs Iterations for PCG and myCG}
    \label{fig:residuals_comparison}
\end{figure}

As it can be seen from the Figures \ref{fig:deblurred_comparison}, the blurred image is deblurred using both the methods. However in our implementation the blurring using pcg is better than the blurring using myCG. This can be seen by comparing the residual graphs for both of them in Figure \ref{fig:residuals_comparison} which the residual for pcg is much lower than the residual for myCG. The main reason for why this happens is because of the preconditioner. The preconditioner is used to reduce the condition number of the system, which makes the system well-conditioned and hence the solution is more accurate. Hence the blurring using pcg performs better than the blurring using myCG, but it should also be noted that the pcg method is more computationally expensive than the myCG method. Therefore if we want a more accurate solution and we are willing to pay the computational cost, then we should use the pcg method. However if we want a less accurate solution and we are not willing to pay the computational cost, then we should use the myCG method.
\subsection{When would \texttt{pcg} worth the added computational cost? What about if you are deblurring lots of images with the same blur operator?}

There are several scenarios in which \texttt{pcg} is worth the added computational cost. One of the scenarios is when the system is ill-conditioned. In this case the solution obtained by \texttt{pcg} will be more accurate than the solution obtained by \texttt{myCG}. Another scenario is when the system is large. In this case the \texttt{pcg} will be faster than \texttt{myCG} because of the preconditioner. The preconditioner reduces the condition number of the system, hence the system is well-conditioned and the solution is more accurate. Hence in this case \texttt{pcg} is worth the added computational cost.
\\
\\
If we are deblurring lots of images with the same blur operator, then \texttt{pcg} is worth the added computational cost. This is because the preconditioner is computed only once and then used for all the images. Therefore the computational cost is reduced.


\section{Reproducing the results}
In order to reproduce the results, Matlab is needed to be installed. The code is in the directory \texttt\texttt{Project4\_FilesData} and inside that directory there is a folder called \texttt{graphs}  which contains all the graphs. There would be two more directories called \texttt{blur\_data} and \texttt{deblur\_data} which contain the data for the project. The file \texttt{code\_template.m} must be run in order to reproduce the results. It uses the file \texttt{myCG.m} which is the implementation of the Conjugate Gradient Algorithm.
\\

Bellow is the visualization of the directory structure:

\dirtree{%
    .1 Project\_4\_sawhney\_harkeerat.
    .2 blur\_data.
    .3 A\_test.mat.
    .3 b\_test.mat.
    .3 x\_test\_exact.mat.
    .2 test\_data.
    .3 A.mat.
    .3 B.mat.
    .2 graphs.
    .3 blurred.png.
    .3 deblurred\_mycg.png.
    .3 deblurred\_pcg.png.
    .3 residuals\_mycg.png.
    .3 residuals\_pcg.png.
    .2 code\_template.m.
}



\end{document}
